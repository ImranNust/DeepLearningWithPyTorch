{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0jxzxN0usvhViDN7qdixD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a101061b950477797a54b09520654a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bea3e2b86a034a1fbe281f7c4e6ccfce",
              "IPY_MODEL_2c388eb5b75947b7ba5ddc33c54a4309",
              "IPY_MODEL_acd688dc0e604ad588178280b81f62b8"
            ],
            "layout": "IPY_MODEL_0533b43e924d467892c5b53526702ca2"
          }
        },
        "bea3e2b86a034a1fbe281f7c4e6ccfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01bd3acf46243ff92d9bc48f15eef26",
            "placeholder": "​",
            "style": "IPY_MODEL_d2443ee00629458a83a5558cd9ebebb0",
            "value": "100%"
          }
        },
        "2c388eb5b75947b7ba5ddc33c54a4309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aa93dff83a8401cab26e68674cb3948",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b4363ae310b4d6eb92499b55438bf46",
            "value": 170498071
          }
        },
        "acd688dc0e604ad588178280b81f62b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e29d1ac2815947acbb96c53ea1194623",
            "placeholder": "​",
            "style": "IPY_MODEL_c8f8d52512494b7eaf6db15f5024f025",
            "value": " 170498071/170498071 [00:14&lt;00:00, 15109962.99it/s]"
          }
        },
        "0533b43e924d467892c5b53526702ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01bd3acf46243ff92d9bc48f15eef26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2443ee00629458a83a5558cd9ebebb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aa93dff83a8401cab26e68674cb3948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4363ae310b4d6eb92499b55438bf46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e29d1ac2815947acbb96c53ea1194623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f8d52512494b7eaf6db15f5024f025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImranNust/DeepLearningWithPyTorch/blob/main/Module2_DistinguishingBirdsFromAirplanes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h1> <center> <b> <u> Distinguishing Birds from Airplanes </u> </b> </center> </h1>\n",
        "\n",
        "The first step involves building the dataset. We’ll pick out all the birds and airplanes from our CIFAR-10 dataset and build a neural network that can tell birds and airplanes apart."
      ],
      "metadata": {
        "id": "CYWnfkegnzIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "vvPKWn8MYLFv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "id": "eT-tMaT7YLy3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = '/content/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train = True, download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9a101061b950477797a54b09520654a4",
            "bea3e2b86a034a1fbe281f7c4e6ccfce",
            "2c388eb5b75947b7ba5ddc33c54a4309",
            "acd688dc0e604ad588178280b81f62b8",
            "0533b43e924d467892c5b53526702ca2",
            "f01bd3acf46243ff92d9bc48f15eef26",
            "d2443ee00629458a83a5558cd9ebebb0",
            "5aa93dff83a8401cab26e68674cb3948",
            "4b4363ae310b4d6eb92499b55438bf46",
            "e29d1ac2815947acbb96c53ea1194623",
            "c8f8d52512494b7eaf6db15f5024f025"
          ]
        },
        "id": "VHcC1_49YN4F",
        "outputId": "2bdce585-7fa7-41dd-d888-8f7ba45bb4dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a101061b950477797a54b09520654a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/cifar-10-python.tar.gz to /content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train = False, download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SvzOCCpaSYM",
        "outputId": "64fea7aa-9554-41ce-9b50-6e9ce0cf9551"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h2> <center> <b> <u> Building the Dataset </u> </b> </center> </h2>\n",
        "\n",
        "The first step is to get the data in the right shape. We could create a `Dataset` subclass that only includes birds and airplanes. However, the dataset is small, and we only need indexing and `len` to work on our dataset. It doesn’t actually have to be a subclass of `torch.utils.data.dataset.Dataset`! Well, why not take a shortcut and just filter the data in `cifar10` and remap the labels so they are contiguous? Here’s how:\n"
      ],
      "metadata": {
        "id": "-wHlCSz1rcnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
        "\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
      ],
      "metadata": {
        "id": "H08491_NnQ7u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cifar2 object satisfies the basic requirements for a Dataset—that is,`__len__` and `__getitem__` are defined — so we’re going to use that. We should be aware, however, that this is a clever shortcut and we might wish to implement a proper `Dataset` if we hit limitations with it.\n",
        "\n",
        "We have a dataset! Next, we need a `model` to feed our data to."
      ],
      "metadata": {
        "id": "XEMMznxzsaj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h3> <center> <b> <u> A Fully Connected Model </u> </b> </center> </h3>\n",
        "\n",
        "Our image size is  $32 × 32 × 3$: that is, 3,072 input features per sample. Our new model would be an `nn.Linear` with $3,072$ input features and some number of hidden features, followed by an activation, and then another `nn.Linear` that tapers the network down to an appropriate output number of features (2, for this use case)\n",
        "\n",
        "![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Chapter7/Images/Pic77.png)"
      ],
      "metadata": {
        "id": "qPewUnG7so6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "n_out = 2\n",
        "model = nn.Sequential(\n",
        "    nn.Linear( 3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, n_out)\n",
        "    )"
      ],
      "metadata": {
        "id": "oLSzBAkzsTvD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have a model. Next we’ll discuss what our model output should be."
      ],
      "metadata": {
        "id": "tQkVsOlsuUfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<h3> <center> <b> <u> Representing the output as probabilities </u> </b> </center> </h3>\n",
        "\n",
        "Softmax is a function that takes a vector of values and produces another vector of the same dimension, where the values represent probabilities. The expression for softmax is shown in the below figure.\n",
        "\n",
        "![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Chapter7/Images/Pic78.png)\n",
        "\n",
        "That is, we take the elements of the vector, compute the elementwise exponential,\n",
        "and divide each element by the sum of exponentials. In code, it’s something like this:"
      ],
      "metadata": {
        "id": "uQPRdPlouwsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum()\n",
        "\n",
        "# Let's test it on an input vector\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "print('Input: {}'.format(x))\n",
        "print('Softmax Output: {}'.format(softmax(x)))\n",
        "\n",
        "# As expected, it satisfies the constraints on probability:\n",
        "print(softmax(x).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCI2zp5bvkpt",
        "outputId": "57b445d0-0b28-4631-cbf8-b02e643f2142"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([1., 2., 3.])\n",
            "Softmax Output: tensor([0.0900, 0.2447, 0.6652])\n",
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax is a monotone function, in that lower values in the input will correspond to lower values in the output. However, it’s not scale invariant, in that the ratio between values is not preserved. In fact, the ratio between the first and second elements of the input is 0.5, while the ratio between the same elements in the output is 0.3678. This is not a real issue, since the learning process will drive the parameters of the model in a way that values have appropriate ratios.\n",
        "\n",
        "The `nn` module makes softmax available as a module. Since, as usual, input tensors may have an additional batch 0th dimension, or have dimensions along which they encode probabilities and others in which they don’t, `nn.Softmax` requires us to specify the dimension along which the softmax function is applied:\n",
        "\n"
      ],
      "metadata": {
        "id": "6T21EkCFwNEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "x = torch.tensor([[1.0, 2.0, 3.0],[1.0, 2.0, 3.0]])\n",
        "print(softmax(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kpa0s2wv63w",
        "outputId": "9e31e957-245c-4f0b-a334-d270c638a230"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0900, 0.2447, 0.6652],\n",
            "        [0.0900, 0.2447, 0.6652]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we have two input vectors in two rows (just like when we work with\n",
        "batches), so we initialize nn.Softmax to operate along dimension 1.\n",
        "\n",
        "\n",
        "Excellent! We can now add a `softmax` at the end of our model, and our network\n",
        "will be equipped to produce probabilities:"
      ],
      "metadata": {
        "id": "4CWlUzoMwnUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 2),\n",
        "    nn.Softmax(dim = 1)\n",
        "    )"
      ],
      "metadata": {
        "id": "cDreVy_iwh13"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can actually try running the model before even training it. Let’s do it, just to see\n",
        "what comes out. We first build a batch of one image, our bird (figure 7.9):\n",
        "```\n",
        "img, _ = cifar2[0]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "Jdj-ixcNxJ5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, _ = cifar2[0]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "plt.axis(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eibzRviBw35u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "3df6598e-5bab-4c38-f177-771385fc77ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPiklEQVR4nO3de2yW5RnH8atYoShg0aIgyCiCE50gOPA0yAARXXQOPG7q8DAPEzMyl+nM3OZUnC7DAxGji2boIg6d5ygoVDaZMMEjjVYHCAoCUoSKFYFUuj/89/n9Ml/bejX7fv68f7mf92npxZs893PfV1lzc3MAyKfD130DAIpRnEBSFCeQFMUJJEVxAkmVu/DeKJOPcn9yvJlYXTzcbZD+uB7le8ns8MH7yGzCyAtlNq5scuH4vrGnnLMo5svsypqTZHb02B0y07Mieojx5WaO+PVGREQXk203WaMYH2HmlGqXyZ4S46vNnLroLbOmaJLZ/JoPZfZenfnA10ymPKOj5vXNZUXjfHMCSVGcQFIUJ5AUxQkkRXECSVGcQFJ2KcU9lo9TTXZp8fDWQfqx9tbDPpLZqg06W7Hkbp2NKf7xfjhstJxzTOglnTvHVspseejH8i/LRC9v9DFz9G8xYpPJ9N2XumTS32SDZfJyLJXZH574oHC8ywHmo6r0T10zXS9xdRxqrqlvMaLBZIpbxxL45gSSojiBpChOICmKE0iK4gSSojiBpMrcGULfMbtSKsxFax4oHh8y1D271sslb1z9vp72pLnkicXDp16rH/OPG7FMZu4Juvt9rDaZemIvbj0iIr5tsqNN1i32M6n6/buFG70bZFFMldm8JxbI7Nof3FccTNR3ceQdOnvpOZ3FqyZzO0/UAqT+sazmZnalAO0KxQkkRXECSVGcQFIUJ5CUffH9xWt0duANOpt8dvH4jEfMIzB3Zstwk7mntXOKhxdP1k9kTzaXc09rbzGZM06Mu2ek7gyhbuYJasS+MrlU/HTj4iA5Z3h0klm92TZRe8A5MosQT2vNL+TgXjprGKmzd1aZ2zDXdOcBtSS+OYGkKE4gKYoTSIriBJKiOIGkKE4gKbuUYt5djpVm2ozvi8C9HW4OuDnQtH5Y6ZZgZhUPrzMH7Zz3grmeuf99S+xbUCXG1RJLRMRB5pyjiN1k8nSsk9l28Q9QHXqzwuLQZzGd5Q6ZGqYj+ZP3mydnzHtLX23dDPNRa022xmSqd0UL45sTSIriBJKiOIGkKE4gKYoTSIriBJLySymOOCcoIvQOk8PMnAt01GSO4rfnx6gj8F3baMfsgGm8VWeX9dVZvRhfaG6jNj6WWT+TvW6uOVycL7Ql/iHnPBin6wsWnorzvziteLhJdyNfN+NxfTm386S7ydQ/TBvimxNIiuIEkqI4gaQoTiApihNIiuIEkrLtGMrKdDsG63oxfr+Z4/oImN0g39SNreMiMf4t81GbzAFZFy8p7rocEbGtVl9zyIU6Uxsc3K6fY032U5OpHTAREb2ieL2nNj6Xc85ZZtaIhpiTteKPJiuB+8HGmMy1bl9sMrVjpcTdKrRjANoZihNIiuIEkqI4gaQoTiApihNIqnWWUlRb5q5mjtspssVk6jCxiIhRYtz0XjnVrA6YM8jiXtcl2R0WJQ4GO9T06jB7QewykdncE/3EeF3sI+d8t+YQfcHj1JagCN3PO/TyxiBzuTNM5g6AW28y0WenNbCUArQzFCeQFMUJJEVxAklRnEBSpZ8h5GauEOM/LvGzHjLZoyZTJ/j301MemWyuZ1oudNBdC6KnaT8wUIyfa25jgMkcdyyOypriIz1pnu56bc00T2uFUybprKeZd/clJtQdHlLgmxNIiuIEkqI4gaQoTiApihNIiuIEkip9KaXJZJ+IcfcSsuM6YrufQLWHdi/gbzDZfeY2puhs5O7mmoJpvm27Xjul/Prd6+txs34pPsxZTHdNOkFmA2Ju4bj7faw2mZ3o/oYT4JsTSIriBJKiOIGkKE4gKYoTSIriBJIqfSnFUcsRs8wcs6tDbt2I8GcPqc7FbneM2wFjjtvfac4JWt1fZ33EeIXoNB0RMSc+lJk75+gpk6mNRJ77NN3PoJ85Q0j9k7n7awq9O2bIlP/I7A3Xaf33JlN/q27Jr4fJBL45gaQoTiApihNIiuIEkqI4gaQoTiCp1mnH0NLcCU5uF4l6VH6xmdPZZGY7iGvjcHrsaS5a3H+gyiylbIplMvu3+aTbPjPhNDHuupEvv0dng/RJaWe+tUNmqh/2wTFYzhket8usKXTr83LRzTsiYmkcKLMN4vffGHrZ5u1m3av81rKVtGMA2hOKE0iK4gSSojiBpChOICmKE0iqdXaltCW3E6BWjF9t5vxJR5PMconaTRERsdp0h64Sj+zLzT/N6+azbnvChM+bTB2E5Xb9xLs6Gqf/YRpCL6WolbEuZvmoPK6UWa94X2YHyfWjiLFxtszUUWmb4wM5Y++y48z1ivHNCSRFcQJJUZxAUhQnkBTFCSRFcQJJte1Sivs017eiwWS2mYdgDurySwd6LaU8JpiP0ydJ9RG7LTaZdu8LX5VRxGLTS73F+4bcKJMju3eS2c/NFdUtugO+FpoDw9yfx3Vxjsz6m2tG7F84ujQ+lTPGx2hzvd0KR/nmBJKiOIGkKE4gKYoTSIriBJJq26e1pXYSLuWJbKmKj/T5Itqin05+tv0smQ2oKn4aFxHyX6DcPFG+aJjuDH3+MD1vRWyUWe2zxS+xP/3QzfqC8bhMRjbpl9vHxySZTRPtw0vtdLDeZKtM1secS6T+adz5TS9/NlNmv+78XuE435xAUhQnkBTFCSRFcQJJUZxAUhQnkFT7P0OoFGa5ZO/G38rs4Rn6iP4elXq5pMF05m4Uh+asWK6XIvoN1C+VV5hm0yPH7CuznscUZ3MmTpFzdj2ql1IWm3WKt8RySUTE4WK8OnrLOWvM2T1dzZ94k3jhPCLiL+acI9WN/EQ5I6Kis24nofDNCSRFcQJJUZxAUhQnkBTFCSRFcQJJtelSyv799BJA9agRMivfrm/znw8t+PI3Un2FjDavUr2VI6K+ePdARMTGgbp79foNehlgc63ohrzsTTnnzUZ9Vk00fiyjR4YPlVnHocXLRLseNWcSGS+qVhgRcaeZ11WM15vlkkHmeuPMVqhKk7ljq9SJUCPC7eC51GTF+OYEkqI4gaQoTiApihNIiuIEkqI4gaTsUspNF1wns4pR+rF8xdBDCsdHV/eXc7qoZ+hhN5HExJ6Xy6xm+t+KA7V8ERFRqzshRxe9XBKbdI+EzfX7mXmqO7ReOgjTKTvC9GpYqHfc7FyorrmX+SzDLKW489rmivGVN5hJ7hQvc+DZJRfq7F/mkur+jzEHntkbieKu13xzAklRnEBSFCeQFMUJJEVxAklRnEBSZc3NzS63YQa9zz5PZutm6YOkWp5b3jhNR5VHF483vGWuZ5Z7YqHJ3PJM2znU/FWpfSLv3GQueLXJ3JaV602mm5FHVBcPH7G7njLVXG58NJcVjfPNCSRFcQJJUZxAUhQnkBTFCSTV7p/Wdhqs+w/srNXn6eSh3r52bcDb8il0idyWir+bbFTx8BHd9ZRXJpvriXYXERFhjouKS0ymXrRfq6ecIn6uiIjHeVoLtC8UJ5AUxQkkRXECSVGcQFIUJ5BUmqWUnbFOZh3jI5ntWaY7Bm/7SneEr+Qyk5mze+KA4uGOZmlGvIceERGVn+isziyzVOjuILGxvnj80Ao9p4+5/7mdWUoB2hWKE0iK4gSSojiBpChOICmKE0iqVTpbbxbjjaFaD0Q0NM+XWc/4UGYsl+R05AydvfSszrqJrgXuD3W9OVLp/L56qW1C32Uyc3uCrhHTjhqr57gjiRS+OYGkKE4gKYoTSIriBJKiOIGkKE4gqVZZStlbjHcJ3dl6w/O6VcCcTbrFwB6m7fW2Rp2hBZxY4rzXdNR9fPH4FnO5CX11dnp0kpnZRBILTHbsmOJxsaEmIiIeXKSzKccUj/PNCSRFcQJJUZxAUhQnkBTFCSRFcQJJtcpSSimqqnvLrN+YoTIbWquXWV6cWry34Iir9H28oiP/7H25yWa5i7Yh0UQ7IiIWl3C9a3Q0LvaS2eG/0n92K8RhbkvNUXPbC4/H+sItsVRmbiXItD2JkeLz6s09rlllLshSCtC+UJxAUhQnkBTFCSRFcQJJ2ae17nyexs90VtlZfdinck7//vql+MZPXpCZeiLr1N1twu+ZTBzDHxERA7/0bbS9hhLm9DGZaXVwwxjTVXyQuaZ4AtzBbHCY7Z6Ems0Pc8VT0oiIE8wlVUPsBvPUuGGiuaDANyeQFMUJJEVxAklRnEBSFCeQFMUJJFVyZ+v31+tJfXsVj2+M2XLOw7PPktnlOrL/u+wy2f8lt9yjXtx3c8xSSpiu0dZMMe4OEXIv7Z9hMrcEo/dTxD1PFI+fG3rzxhWhz8i6I+hsDbQrFCeQFMUJJEVxAklRnEBSFCeQlF1KWRR1Mly/ZKWcVyEesf91wclyzuznZBTmGJj24Wcmm97Cn/U7HXU07ZV3ntbC9+G45Q215cMtzUw1mdnN4nasOEPEdq0/i91YEREjdRPt2DGYpRSgXaE4gaQoTiApihNIiuIEkqI4gaTsAV/z1s+UWe2iB2TWuLz4DfwFpqOx3SHQzo09U2c1Lb2Ucp+OdroDvoaL8dZYxqo2mWoPvXuJn1Vqd3OzG+cN8Xf8mDkwrMq1vRb45gSSojiBpChOICmKE0iK4gSSojiBpOxSyvIlermkvKs+sKhqRPH4SNMjo+aXOuumo9hqMmW8aWn87JwSLhgRY9VSREQM1Y25o0btWCl1iWW1ySpNppYOXCsatzTmfPn2NhGjTfYjk5XaVdx1Khe9dm4yB54NGf/lb4FvTiApihNIiuIEkqI4gaQoTiApihNIyh7wddQlZTLsYt6yXysWaHqa5YZK83h90yadNZj72C661debx+tb63RWMnfAV4/i4T3EeETEtmvN9czhWYeerbMBYvmrwnzUY6JnSETEzmfMRNd2vqsYN3878aTJfmOyUqmD0o7WUzr8QmefH8QBX0C7QnECSVGcQFIUJ5AUxQkkZV98P9+co/K2eUqq3q9u6q7n9Bymsw2v6qzOPHndNU1nbcqdY3N/8fA2c97PEdfrbE29zt682WTHF4/vYV7av/EUndWZbL5ppv7eQyIwndRDdFKPCP+Ut9TzhVyXbaHcPfYW+OYEkqI4gaQoTiApihNIiuIEkqI4gaTsi+8br9Evvq+9qpOc9+C0HYXjs8zL0A3mZehK8xi9YZ7ORAPi1lFlMvc4v8QziyTVGToiYruOuomllK2mTcY3LtDZSWN1Zt7pj+nvFo9vvt1MMstwYZaWbAsQd5OPinF3NtJdOmoexYvvQLtCcQJJUZxAUhQnkBTFCSRFcQJJ2aUUAF8fvjmBpChOICmKE0iK4gSSojiBpChOIKn/AhIv2NuOsPtmAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to call the model, we need to make the input have the right dimensions. We recall that our model expects $3,072$ features in the input, and that nn works with data organized into batches along the zeroth dimension. So we need to turn our $3 × 32 × 32$ image into a 1D tensor and then add an extra dimension in the zeroth position. \n",
        "\n"
      ],
      "metadata": {
        "id": "sz5aBOeeLdTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_batch = img.view(-1).unsqueeze(0)\n",
        "\n",
        "# Now we are ready to invoke our model:\n",
        "\n",
        "out = model(img_batch)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppqZIIQ5VGuL",
        "outputId": "29ae0fcc-0bbf-4de8-c38a-27f0d64775ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4532, 0.5468]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we got probabilities! \n",
        "\n",
        "In addition, while we know which output probability is supposed to be which (recall our class_names), our network has no indication of that. Is the first entry “airplane” and the second “bird,” or the other way around? The network can’t even tell that at this point. It’s the loss function that associates a meaning with these two numbers, after backpropagation. If the labels are provided as index 0 for “airplane” and index 1 for “bird,” then that’s the order the outputs will be induced to take. Thus, after training, we will be able to get the label as an index by computing the `argmax` of the output probabilities: that is, the index at which we get the maximum probability. \n",
        "\n",
        "Conveniently, when supplied with a dimension, `torch.max` returns the maximum element along that dimension as well as the index at which that value occurs. In our case, we need to take the max along the probability vector (not across batches), therefore, dimension 1:"
      ],
      "metadata": {
        "id": "xK_CiSz0MNLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, index = torch.max(out, dim = 1)\n",
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYPibVuxVLMr",
        "outputId": "6044f69b-a1fd-4cdf-df5e-e882c962ddf5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It says the image is a bird. "
      ],
      "metadata": {
        "id": "rPzBNivvN36J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h3> <center> <b> <u> A Loss For Classifying </u> </b> </center> </h3>\n",
        "\n",
        "- Loss is what gives probabilities meaning. \n",
        "\n",
        "- Our loss for classification can be computed as follows. For each sample\n",
        "in the batch:\n",
        "  1. Run the forward pass, and obtain the output values from the last (linear) layer.\n",
        "  2. Compute their softmax, and obtain probabilities.\n",
        "  3. Take the predicted probability corresponding to the correct class (the likelihood of the parameters). Note that we know what the correct class is because it’s a supervised problem—it’s our ground truth.\n",
        "  4. Compute its logarithm, slap a minus sign in front of it, and add it to the loss.\n",
        "\n",
        "- **PyTorch** has an `nn.NLLLoss` class. \n",
        "  - However, as opposed to what you might expect, it does not take probabilities but rather takes a tensor of log probabilities as input. \n",
        "  - It then computes the NLL of our model given the batch of data. \n",
        "  - There’s a good reason behind the input convention: taking the logarithm of a probability is tricky when the probability gets close to zero. The workaround is to use `nn.LogSoftmax` instead of `nn.Softmax`, which takes care to make\n",
        "the calculation numerically stable.\n",
        "\n",
        "\n",
        "We can now modify our model to use `nn.LogSoftmax` as the output module:"
      ],
      "metadata": {
        "id": "M4ZZPUBsN_eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 2),\n",
        "    nn.LogSoftmax(dim = 1)\n",
        ")\n",
        "\n",
        "# Then we instantiate our NLL loss:\n",
        "\n",
        "loss = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "IwiQ7kRWViA1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss takes the output of `nn.LogSoftmax` for a batch as the first argument and a tensor of class indices (zeros and ones, in our case) as the second argument. We can now test it with our birdie:"
      ],
      "metadata": {
        "id": "M17g8J9hQYSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = cifar2[0]\n",
        "out = model(img.view(-1).unsqueeze(0))\n",
        "loss(out, torch.tensor([label]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIE-dx2yQhUL",
        "outputId": "12d4d653-3597-486f-f6f3-1f547a4c2d0d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5285, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ending our investigation of losses, we can look at how using cross-entropy loss\n",
        "improves over MSE. In the figure below, we see that the cross-entropy loss has some slope when the prediction is off target (in the low-loss corner, the correct class is assigned a predicted probability of 99.97%), while the MSE we dismissed at the beginning saturates much earlier and—crucially—also for very wrong predictions. \n",
        "\n",
        "The underlying reason is that the slope of the MSE is too low to compensate for the flatness of the softmax function for wrong predictions. This is why the MSE for probabilities is not a good fit for classification work.\n",
        "\n",
        "![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Chapter7/Images/Pic711.png)"
      ],
      "metadata": {
        "id": "5g6Lx49tQ1fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h3> <center> <b> <u> Training the Classifier </u> </b> </center> </h3>\n",
        "\n",
        "\n",
        "Let's train the classifier."
      ],
      "metadata": {
        "id": "CjWNrobZSA2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 2),\n",
        "    nn.LogSoftmax(dim = 1)\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for img, label in cifar2:\n",
        "    out = model(img.view(-1).unsqueeze(0))\n",
        "    loss = loss_fn(out, torch.tensor([label]))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  \n",
        "  print(\"Epoch: {} | Loss: {} \".format(epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQNAuQ93QwZs",
        "outputId": "b5ae1b55-7694-4f9b-921b-ab149ca00b81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 5.091197967529297 \n",
            "Epoch: 1 | Loss: 7.387642860412598 \n",
            "Epoch: 2 | Loss: 6.784652233123779 \n",
            "Epoch: 3 | Loss: 9.931572914123535 \n",
            "Epoch: 4 | Loss: 5.155850887298584 \n",
            "Epoch: 5 | Loss: 4.181996822357178 \n",
            "Epoch: 6 | Loss: 3.1013920307159424 \n",
            "Epoch: 7 | Loss: 7.673023223876953 \n",
            "Epoch: 8 | Loss: 5.854520320892334 \n",
            "Epoch: 9 | Loss: 8.945448875427246 \n",
            "Epoch: 10 | Loss: 17.048053741455078 \n",
            "Epoch: 11 | Loss: 5.790252685546875 \n",
            "Epoch: 12 | Loss: 7.602874279022217 \n",
            "Epoch: 13 | Loss: 15.614364624023438 \n",
            "Epoch: 14 | Loss: 4.664062023162842 \n",
            "Epoch: 15 | Loss: 3.3730580806732178 \n",
            "Epoch: 16 | Loss: 7.134612083435059 \n",
            "Epoch: 17 | Loss: 20.795635223388672 \n",
            "Epoch: 18 | Loss: 10.340953826904297 \n",
            "Epoch: 19 | Loss: 6.510754108428955 \n",
            "Epoch: 20 | Loss: 14.100193977355957 \n",
            "Epoch: 21 | Loss: 2.701233148574829 \n",
            "Epoch: 22 | Loss: 16.944007873535156 \n",
            "Epoch: 23 | Loss: 11.111300468444824 \n",
            "Epoch: 24 | Loss: 6.765738487243652 \n",
            "Epoch: 25 | Loss: 19.840011596679688 \n",
            "Epoch: 26 | Loss: 11.464639663696289 \n",
            "Epoch: 27 | Loss: 0.41630032658576965 \n",
            "Epoch: 28 | Loss: 7.571674346923828 \n",
            "Epoch: 29 | Loss: 1.2756690979003906 \n",
            "Epoch: 30 | Loss: 5.9280829429626465 \n",
            "Epoch: 31 | Loss: 2.5655357837677 \n",
            "Epoch: 32 | Loss: 11.263482093811035 \n",
            "Epoch: 33 | Loss: 5.923294544219971 \n",
            "Epoch: 34 | Loss: 0.4080386161804199 \n",
            "Epoch: 35 | Loss: 7.496790885925293 \n",
            "Epoch: 36 | Loss: 4.79972505569458 \n",
            "Epoch: 37 | Loss: 4.1455888748168945 \n",
            "Epoch: 38 | Loss: 4.96770715713501 \n",
            "Epoch: 39 | Loss: 0.3048759698867798 \n",
            "Epoch: 40 | Loss: 0.07047852873802185 \n",
            "Epoch: 41 | Loss: 2.351503372192383 \n",
            "Epoch: 42 | Loss: 9.141670227050781 \n",
            "Epoch: 43 | Loss: 15.548713684082031 \n",
            "Epoch: 44 | Loss: 4.364133358001709 \n",
            "Epoch: 45 | Loss: 5.547656059265137 \n",
            "Epoch: 46 | Loss: 10.8107328414917 \n",
            "Epoch: 47 | Loss: 9.965847969055176 \n",
            "Epoch: 48 | Loss: 1.4365891218185425 \n",
            "Epoch: 49 | Loss: 6.120819091796875 \n",
            "Epoch: 50 | Loss: 0.02823965810239315 \n",
            "Epoch: 51 | Loss: 3.475407838821411 \n",
            "Epoch: 52 | Loss: 5.927113056182861 \n",
            "Epoch: 53 | Loss: 5.088309288024902 \n",
            "Epoch: 54 | Loss: 0.14256015419960022 \n",
            "Epoch: 55 | Loss: 8.193500518798828 \n",
            "Epoch: 56 | Loss: 12.227889060974121 \n",
            "Epoch: 57 | Loss: 14.262269020080566 \n",
            "Epoch: 58 | Loss: 4.01509952545166 \n",
            "Epoch: 59 | Loss: 0.029190663248300552 \n",
            "Epoch: 60 | Loss: 2.4717061519622803 \n",
            "Epoch: 61 | Loss: 9.702754020690918 \n",
            "Epoch: 62 | Loss: 13.036781311035156 \n",
            "Epoch: 63 | Loss: 15.797967910766602 \n",
            "Epoch: 64 | Loss: 12.507013320922852 \n",
            "Epoch: 65 | Loss: 19.934314727783203 \n",
            "Epoch: 66 | Loss: 19.971233367919922 \n",
            "Epoch: 67 | Loss: 13.803239822387695 \n",
            "Epoch: 68 | Loss: 22.44573211669922 \n",
            "Epoch: 69 | Loss: 18.565494537353516 \n",
            "Epoch: 70 | Loss: 5.910828113555908 \n",
            "Epoch: 71 | Loss: 16.8968505859375 \n",
            "Epoch: 72 | Loss: 9.41030502319336 \n",
            "Epoch: 73 | Loss: 10.288349151611328 \n",
            "Epoch: 74 | Loss: 11.66308879852295 \n",
            "Epoch: 75 | Loss: 11.074972152709961 \n",
            "Epoch: 76 | Loss: 6.982987403869629 \n",
            "Epoch: 77 | Loss: 8.870816230773926 \n",
            "Epoch: 78 | Loss: 6.664762020111084 \n",
            "Epoch: 79 | Loss: 12.796662330627441 \n",
            "Epoch: 80 | Loss: 14.255189895629883 \n",
            "Epoch: 81 | Loss: 14.04021167755127 \n",
            "Epoch: 82 | Loss: 0.6877982020378113 \n",
            "Epoch: 83 | Loss: 0.3007162809371948 \n",
            "Epoch: 84 | Loss: 11.922783851623535 \n",
            "Epoch: 85 | Loss: 11.350382804870605 \n",
            "Epoch: 86 | Loss: 10.278024673461914 \n",
            "Epoch: 87 | Loss: 8.618818283081055 \n",
            "Epoch: 88 | Loss: 10.904363632202148 \n",
            "Epoch: 89 | Loss: 13.052075386047363 \n",
            "Epoch: 90 | Loss: 8.418453216552734 \n",
            "Epoch: 91 | Loss: 3.3410136699676514 \n",
            "Epoch: 92 | Loss: 6.58529806137085 \n",
            "Epoch: 93 | Loss: 5.6262993812561035 \n",
            "Epoch: 94 | Loss: 7.243320465087891 \n",
            "Epoch: 95 | Loss: 3.686763286590576 \n",
            "Epoch: 96 | Loss: 7.028881549835205 \n",
            "Epoch: 97 | Loss: 0.10523320734500885 \n",
            "Epoch: 98 | Loss: 0.28555572032928467 \n",
            "Epoch: 99 | Loss: 5.157771587371826 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Chapter7/Images/Pic712.png)\n",
        "\n",
        "\n",
        "- Looking more closely, we made a small change to the training loop. \n",
        "  - Evaluating all 10,000 images in a single batch would be too much, so we decided to have an inner loop where we evaluate one sample at a time and backpropagate over that single sample.\n",
        "  - While in the first case the gradient is accumulated over all samples before being applied, in this case we apply changes to parameters based on a very partial estimation of the gradient on a single sample. \n",
        "  - However, what is a good direction for reducing the loss based on one sample might not be a good direction for others. By shuffling samples at each epoch and estimating the gradient on one or (preferably, for stability) a few samples at a time, we are effectively introducing randomness in our gradient descent.\n",
        "  - Remember SGD? It stands for stochastic gradient descent, and this is what the S is about: working on small batches (aka minibatches) of shuffled data. It turns out that following gradients estimated over minibatches, which are poorer approximations of gradients estimated across the whole dataset, helps convergence and prevents the optimization process from getting stuck in local minima it encounters along the way. \n",
        "  \n",
        "  \n",
        "  - As depicted in the following figure, gradients from minibatches are randomly off the ideal trajectory, which is part of the reason why we want to use a reasonably small learning rate. \n",
        "\n",
        "![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Chapter7/Images/Pic713.png)\n",
        "\n",
        "\n",
        "  - Shuffling the dataset at each epoch helps ensure that the sequence of gradients estimated over minibatches is representative of the gradients computed across the full dataset."
      ],
      "metadata": {
        "id": "tDvEIyTyYLN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In our training code, we chose minibatches of size 1 by picking one item at a time from the dataset. \n",
        "- The `torch.utils.data` module has a class that helps with shuffling and\n",
        "organizing the data in minibatches: **`DataLoader`**. \n",
        "  - The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose from different sampling strategies. \n",
        "  - A very common strategy is uniform sampling after shuffling the data at each\n",
        "epoch. \n",
        "  - Following figure shows the data loader shuffling the indices it gets from the `Dataset`.\n",
        "\n",
        "  ![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Chapter7/Images/Pic714.png)\n",
        "\n",
        "  - Let's see how it is done!\n",
        "    - At a minimum, the `DataLoader` constructor takes a `Dataset` object as input, along with `batch_size` and a `shuffle` Boolean that indicates whether the data needs to be shuffled at the beginning of each epoch:\n",
        "```\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "shuffle=True)\n",
        "```\n",
        " - A `DataLoader` can be iterated over, so we can use it directly in the inner loop of our new training code:\n"
      ],
      "metadata": {
        "id": "wZeifK26zUfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 64, shuffle = True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 2),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 30\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print('Epoch: {} | Loss: {}'.format(epoch, float(loss)))"
      ],
      "metadata": {
        "id": "Md0lEQt_XnbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fc5ea2-2351-4e56-d758-33c67789580f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.70762038230896\n",
            "Epoch: 1 | Loss: 0.33619025349617004\n",
            "Epoch: 2 | Loss: 0.23642782866954803\n",
            "Epoch: 3 | Loss: 0.5164990425109863\n",
            "Epoch: 4 | Loss: 0.3520466983318329\n",
            "Epoch: 5 | Loss: 0.2545475661754608\n",
            "Epoch: 6 | Loss: 0.37964802980422974\n",
            "Epoch: 7 | Loss: 0.44715356826782227\n",
            "Epoch: 8 | Loss: 0.2303515076637268\n",
            "Epoch: 9 | Loss: 0.3733629584312439\n",
            "Epoch: 10 | Loss: 0.24778276681900024\n",
            "Epoch: 11 | Loss: 0.2415207475423813\n",
            "Epoch: 12 | Loss: 0.36287447810173035\n",
            "Epoch: 13 | Loss: 0.2296326458454132\n",
            "Epoch: 14 | Loss: 0.2518613040447235\n",
            "Epoch: 15 | Loss: 0.1595701426267624\n",
            "Epoch: 16 | Loss: 0.37095513939857483\n",
            "Epoch: 17 | Loss: 0.4326622188091278\n",
            "Epoch: 18 | Loss: 0.4955534040927887\n",
            "Epoch: 19 | Loss: 0.34145739674568176\n",
            "Epoch: 20 | Loss: 0.13626419007778168\n",
            "Epoch: 21 | Loss: 0.34926268458366394\n",
            "Epoch: 22 | Loss: 0.17964135110378265\n",
            "Epoch: 23 | Loss: 0.2869797945022583\n",
            "Epoch: 24 | Loss: 0.10058492422103882\n",
            "Epoch: 25 | Loss: 0.6044036149978638\n",
            "Epoch: 26 | Loss: 0.21336965262889862\n",
            "Epoch: 27 | Loss: 0.10491480678319931\n",
            "Epoch: 28 | Loss: 0.17759805917739868\n",
            "Epoch: 29 | Loss: 0.2726375162601471\n",
            "Epoch: 30 | Loss: 0.2951924502849579\n",
            "Epoch: 31 | Loss: 0.1278754323720932\n",
            "Epoch: 32 | Loss: 0.41447436809539795\n",
            "Epoch: 33 | Loss: 0.19008904695510864\n",
            "Epoch: 34 | Loss: 0.33311015367507935\n",
            "Epoch: 35 | Loss: 0.1812438815832138\n",
            "Epoch: 36 | Loss: 0.036187246441841125\n",
            "Epoch: 37 | Loss: 0.34736016392707825\n",
            "Epoch: 38 | Loss: 0.05847423896193504\n",
            "Epoch: 39 | Loss: 0.05935666710138321\n",
            "Epoch: 40 | Loss: 0.08759148418903351\n",
            "Epoch: 41 | Loss: 0.15953081846237183\n",
            "Epoch: 42 | Loss: 0.19743482768535614\n",
            "Epoch: 43 | Loss: 0.07459548115730286\n",
            "Epoch: 44 | Loss: 0.04518836364150047\n",
            "Epoch: 45 | Loss: 0.1144370436668396\n",
            "Epoch: 46 | Loss: 0.09013669192790985\n",
            "Epoch: 47 | Loss: 0.13907365500926971\n",
            "Epoch: 48 | Loss: 0.04484131559729576\n",
            "Epoch: 49 | Loss: 0.06838537752628326\n",
            "Epoch: 50 | Loss: 0.0501573272049427\n",
            "Epoch: 51 | Loss: 0.15435728430747986\n",
            "Epoch: 52 | Loss: 0.06795931607484818\n",
            "Epoch: 53 | Loss: 0.19480054080486298\n",
            "Epoch: 54 | Loss: 0.0469597727060318\n",
            "Epoch: 55 | Loss: 0.09075139462947845\n",
            "Epoch: 56 | Loss: 0.05835311487317085\n",
            "Epoch: 57 | Loss: 0.11607664078474045\n",
            "Epoch: 58 | Loss: 0.040037624537944794\n",
            "Epoch: 59 | Loss: 0.04880104586482048\n",
            "Epoch: 60 | Loss: 0.03359748050570488\n",
            "Epoch: 61 | Loss: 0.12229040265083313\n",
            "Epoch: 62 | Loss: 0.03113490343093872\n",
            "Epoch: 63 | Loss: 0.042333219200372696\n",
            "Epoch: 64 | Loss: 0.15216520428657532\n",
            "Epoch: 65 | Loss: 0.013507002033293247\n",
            "Epoch: 66 | Loss: 0.05346452072262764\n",
            "Epoch: 67 | Loss: 0.07433436065912247\n",
            "Epoch: 68 | Loss: 0.013265370391309261\n",
            "Epoch: 69 | Loss: 0.0199503805488348\n",
            "Epoch: 70 | Loss: 0.010970890522003174\n",
            "Epoch: 71 | Loss: 0.039287082850933075\n",
            "Epoch: 72 | Loss: 0.017586518079042435\n",
            "Epoch: 73 | Loss: 0.015272372402250767\n",
            "Epoch: 74 | Loss: 0.05563615635037422\n",
            "Epoch: 75 | Loss: 0.019414309412240982\n",
            "Epoch: 76 | Loss: 0.022856665775179863\n",
            "Epoch: 77 | Loss: 0.005708948243409395\n",
            "Epoch: 78 | Loss: 0.026858657598495483\n",
            "Epoch: 79 | Loss: 0.01951209083199501\n",
            "Epoch: 80 | Loss: 0.022021323442459106\n",
            "Epoch: 81 | Loss: 0.015990911051630974\n",
            "Epoch: 82 | Loss: 0.006473316811025143\n",
            "Epoch: 83 | Loss: 0.01578902080655098\n",
            "Epoch: 84 | Loss: 0.03716149926185608\n",
            "Epoch: 85 | Loss: 0.02326965518295765\n",
            "Epoch: 86 | Loss: 0.026351716369390488\n",
            "Epoch: 87 | Loss: 0.006478120572865009\n",
            "Epoch: 88 | Loss: 0.024288104847073555\n",
            "Epoch: 89 | Loss: 0.01008997205644846\n",
            "Epoch: 90 | Loss: 0.01350250095129013\n",
            "Epoch: 91 | Loss: 0.0042408122681081295\n",
            "Epoch: 92 | Loss: 0.0085936039686203\n",
            "Epoch: 93 | Loss: 0.02247004397213459\n",
            "Epoch: 94 | Loss: 0.027964435517787933\n",
            "Epoch: 95 | Loss: 0.01348416693508625\n",
            "Epoch: 96 | Loss: 0.028612803667783737\n",
            "Epoch: 97 | Loss: 0.010213625617325306\n",
            "Epoch: 98 | Loss: 0.03304285556077957\n",
            "Epoch: 99 | Loss: 0.012495262548327446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "- At each inner iteration, `imgs` is a tensor of size $64\\times 3\\times 32\\times 32$ -- that is, a minibatch of sixty-four $(32\\times 32)$ RGB images -- while `labels` is a tensor of size $64$ containing label indices. \n",
        "\n",
        "- We see that the loss decreases somehow, but we have no idea whether it’s low enough. Since our goal here is to correctly assign classes to images, and preferably do that on an independent dataset, we can compute the accuracy of our model on the validation set in terms of the number of correct classifications over the total:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ia0r2ChnbtXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size = 64, shuffle = False)\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for imgs, labels in val_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    outputs = model(imgs.view(batch_size, -1))\n",
        "    _, predicted = torch.max(outputs, dim = 1)\n",
        "    total += labels.shape[0]\n",
        "    correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: {}\".format(correct / total))\n"
      ],
      "metadata": {
        "id": "IPtqIQfu2QG0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "- We can improve the accuracy of our model by including a few more layers, like the one shown below:\n",
        "```\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(3072, 1024),\n",
        "  nn.Tanh(),\n",
        "  nn.Linear(1024, 512),\n",
        "  nn.Tanh(),\n",
        "  nn.Linear(512, 128),\n",
        "  nn.Tanh(),\n",
        "  nn.Linear(128, 2),\n",
        "  nn.LogSoftmax(dim=1))\n",
        "```\n",
        "\n",
        "- The combination of `nn.LogSoftmax` and `nn.NLLLoss` is equivalent to using\n",
        "`nn.CrossEntropyLoss`. \n",
        "- This terminology is a particularity of PyTorch, as the `nn.NLLoss` computes, in fact, the cross entropy but with log probability predictions as inputs where `nn.CrossEntropyLoss` takes scores (sometimes called logits).\n",
        "- Technically, `nn.NLLLoss` is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs.\n",
        "\n",
        "- It is quite common to drop the last `nn.LogSoftmax` layer from the network and use `nn.CrossEntropyLoss` as a loss. Let us try that:\n"
      ],
      "metadata": {
        "id": "zd3E_3X5UD1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 1024),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128, 2))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42_esN10T09H",
        "outputId": "77f18242-0eea-408d-9fcc-fe1c58983626"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.302669\n",
            "Epoch: 1, Loss: 0.460002\n",
            "Epoch: 2, Loss: 0.729458\n",
            "Epoch: 3, Loss: 0.460628\n",
            "Epoch: 4, Loss: 0.482963\n",
            "Epoch: 5, Loss: 0.798006\n",
            "Epoch: 6, Loss: 0.311679\n",
            "Epoch: 7, Loss: 0.413948\n",
            "Epoch: 8, Loss: 0.386372\n",
            "Epoch: 9, Loss: 0.606066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Let's compute training and validation accuracies\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qdV3sYKMcJDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Training Accuracy: %f\" % (correct / total))\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Validation Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHqFPXKccBz5",
        "outputId": "6f9c1ade-037f-4f2b-a6d0-2da5370c4436"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.816000\n",
            "Validation Accuracy: 0.778000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "- PyTorch offers a quick way to determine how many parameters a model has through the `parameters()` method of `nn.Model` (the same method we use to provide the parameters to the optimizer). \n",
        "  - To find out how many elements are in each tensor instance, we can call the `numel` method. \n",
        "  - Summing those gives us our total count.\n",
        "- Depending on our use case, counting parameters might require us to check whether a parameter has `requires_grad` set to `True`, as well. \n",
        "  - We might want to differentiate the number of trainable parameters from the overall model size. \n",
        "- Let’s take a look at what we have right now:\n"
      ],
      "metadata": {
        "id": "LVFlKJcGca09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n",
        "print('Total Learnable Parameters: {}\\nParameters of Each Layer: {}'.\n",
        "      format(sum(numel_list), numel_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Kw2i0wdrH1",
        "outputId": "81f99cd3-aa72-49b5-f45d-8922051989a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Learnable Parameters: 3737474\n",
            "Parameters of Each Layer: [3145728, 1024, 524288, 512, 65536, 128, 256, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "In our model, the first layer `(nn.Linear(3072, 1024)` has $3072$ input features and $1024$ output neurons. We also know that a linear layer computes $y = weight * x + bias$, and if $x$ has length 3,072 (disregarding the batch dimension for simplicity) and $y$ must have length $1,024$, then the weight tensor needs to be of size $1,024 × 3,072$ and the bias size must be $1,024$. And $1,024 * 3,072 + 1,024$ = 3,146,752$. We can verify these quantities directly:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FJsDjMiHe_Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear = nn.Linear(3072, 1024)\n",
        "print('Weight Shape: {}\\nBias Shape: {}'.\n",
        "      format(linear.weight.shape, linear.bias.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZvCyGKMen6T",
        "outputId": "d1df6c9b-9da1-436f-fb1f-cb6b2be816f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Shape: torch.Size([1024, 3072])\n",
            "Bias Shape: torch.Size([1024])\n"
          ]
        }
      ]
    }
  ]
}