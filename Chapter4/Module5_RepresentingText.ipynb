{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlnWNhqOq74pVX6WjCG4Id",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImranNust/DeepLearningWithPyTorch/blob/main/Chapter4/Module5_RepresentingText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> <center> <b> <u> Introduction </u> </b> </center> </h1>\n",
        "\n",
        "Our goal in this module is to turn text into something a neural network can process:a tensor of numbers. If we can do that and later choose the right architecture for our text-processing job, we’ll be in the position of doing NLP with PyTorch. We see right away how powerful this all is: we can achieve"
      ],
      "metadata": {
        "id": "K65QXURWnkQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h2> <center> <b> <u> Converting Text to Numbers </u> </b> </center> </h2>\n",
        "\n",
        "There are two particularly intuitive levels at which networks operate on text: at the character level, by processing one character at a time, and at the word level, where individual words are the finest-grained entities to be seen by the network. The technique with which we encode text information into tensor form is the same whether we operate at the character level or the word level. And it’s not magic, either. We stumbled upon it earlier: one-hot encoding.\n",
        "\n",
        "Let’s start with a character-level example. First, let’s get some text to process.\n",
        "\n",
        "**Please read the original book for more information regarding freely available text datasets**"
      ],
      "metadata": {
        "id": "nNhTlH9YrTzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "try:\n",
        "  print(\"[INFO] Required Folder/Files Already Exists...\")\n",
        "  dir_path = \"/content/1342-0.txt\"\n",
        "  with open(dir_path, encoding = 'utf8') as f:\n",
        "    text = f.read()\n",
        "except:\n",
        "  print(\"[INFO] Required Folder/Files Do Not Exist...; therefore, dowonloading it.. cheers...\")\n",
        "  !git clone https://github.com/ImranNust/DeepLearningWithPyTorch\n",
        "  !mv DeepLearningWithPyTorch/Images/1342-0.txt /content/\n",
        "  !rm -rf DeepLearningWithPyTorch\n",
        "  dir_path = \"/content/1342-0.txt\"\n",
        "  with open(dir_path, encoding = 'utf8') as f:\n",
        "    text = f.read()\n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYwvgkwGpJDV",
        "outputId": "bfd03935-f02a-408f-ad15-ca1b860d2107"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Required Folder/Files Already Exists...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h2> <center> <b> <u> One-Hot-Encoding Characters </u> </b> </center> </h2>\n",
        "\n",
        "There’s one more detail we need to take care of before we proceed: **encoding**. This is a pretty vast subject, and we will just touch on it. Every written character is represented by a code: a sequence of bits of appropriate length so that each character can be uniquely identified. The simplest such encoding is ASCII (American Standard Code for Information Interchange), which dates back to the 1960s. ASCII encodes 128 characters\n",
        "using 128 integers. For instance, the letter `a` corresponds to binary `1100001` or decimal `97`, the letter `b` to binary `1100010` or decimal `98`, and so on. The encoding fits 8 bits, which was a big bonus in 1965.\n",
        "\n",
        "----\n",
        "\n",
        "**Please Note:** NOTE 128 characters are clearly not enough to account for all the glyphs, accents, ligatures, and so on that are needed to properly represent written text in languages other than English. To this end, a number of encodings\n",
        "have been developed that use a larger number of bits as code for a wider\n",
        "range of characters. That wider range of characters was standardized as Unicode,\n",
        "which maps all known characters to numbers, with the representation in bits of those numbers provided by a specific encoding. Popular encodings are UTF-8, UTF-16, and UTF-32, in which the numbers are a sequence of 8-, 16-, or 32-bit integers, respectively. Strings in Python 3.x are Unicode strings.\n",
        "\n",
        "---\n",
        "\n",
        "We are going to one-hot encode our characters. It is instrumental to limit the one-hot encoding to a character set that is useful for the text being analyzed. In our case, since we loaded text in English, it is safe to use ASCII and deal with a small encoding. We could also make all of the characters lowercase, to reduce the number of different characters in our encoding. Similarly, we could screen out punctuation, numbers, or other characters that aren’t relevant to our expected kinds of text. This may or may not make a practical difference to a neural network, depending on the task at hand.\n",
        "\n",
        "At this point, we need to parse through the characters in the text and provide a\n",
        "one-hot encoding for each of them. Each character will be represented by a vector of length equal to the number of different characters in the encoding. This vector will contain all zeros except a one at the index corresponding to the location of the character in the encoding.\n",
        "\n",
        "We first split our text into a list of lines and pick an arbitrary line to focus on:\n",
        "\n"
      ],
      "metadata": {
        "id": "59onswJzzckz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = text.split('\\n')\n",
        "line = lines[200]\n",
        "line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "huSj0vLN7kOa",
        "outputId": "c84e7853-697b-462a-cadd-4b52de74af26"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s create a tensor that can hold the total number of one-hot-encoded characters for the whole line:"
      ],
      "metadata": {
        "id": "C7UixdBt7uRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "letter_t = torch.zeros(len(line), 128) #128 hardcoded due to the limites of ASCII\n",
        "letter_t.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLIH7f2g7ph9",
        "outputId": "55094f29-ceca-4c7f-cd01-196fc5d70395"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([70, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `letter_t` holds a one-hot-encoded character per row. Now we just have to set a one on each row in the correct position so that each row represents the correct character. The index where the one has to be set corresponds to the index of the character in the encoding:"
      ],
      "metadata": {
        "id": "WkIrvulL79md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, letter in enumerate(line.lower().strip()):\n",
        "  letter_index = ord(letter) if ord(letter) < 128 else 0 # The text uses directional double qoutes, which are not valid ASCII, so we are screen them out here.\n",
        "  letter_t[i][letter_index] = 1\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "3eXhJhIk72Y4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h2> <center> <b> <u> One-hot Encoding Whole Words </u> </b> </center> </h2>\n",
        "\n",
        "We have one-hot encoded our sentence into a representation that a neural network\n",
        "could digest. Word-level encoding can be done the same way by establishing a vocabulary and one-hot encoding sentences—sequences of words—along the rows of our tensor. Since a vocabulary has many words, this will produce very wide encoded vectors, which may not be practical. We will see in the next section that there is a more efficient way to represent text at the word level, using embeddings. For now, let’s stick with one-hot encodings and see what happens.\n",
        "\n",
        "We’ll define `clean_words`, which takes text and returns it in lowercase and stripped of punctuation. When we call it on our “Impossible, Mr. Bennet” `line`, we get the following:"
      ],
      "metadata": {
        "id": "1OgqQyA58_62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_words(input_str):\n",
        "  punctuation = '.,;:\"!?”“_-'\n",
        "  word_list = input_str.lower().replace('\\n', ' ').split()\n",
        "  word_list = [word.strip(punctuation) for word in word_list]\n",
        "  return word_list"
      ],
      "metadata": {
        "id": "nyZjqTRT8bPN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_line = clean_words(line)\n",
        "line, words_in_line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4Qu1eiA8nQK",
        "outputId": "0018ddd3-9f02-4002-efea-7c1ecc0bee4d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him',\n",
              " ['impossible',\n",
              "  'mr',\n",
              "  'bennet',\n",
              "  'impossible',\n",
              "  'when',\n",
              "  'i',\n",
              "  'am',\n",
              "  'not',\n",
              "  'acquainted',\n",
              "  'with',\n",
              "  'him'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s build a mapping of words to indexes in our encoding:\n"
      ],
      "metadata": {
        "id": "7C3xjHsI-H8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = sorted(set(clean_words(text)))\n",
        "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
        "\n",
        "len(word2index_dict), word2index_dict['impossible']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0CLRDqC81MX",
        "outputId": "e5cb942c-33cb-4ef3-bc60-bf0e3e348300"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7261, 3394)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Note that `word2index_dict` is now a dictionary with words as keys and an integer as a value. We will use it to efficiently find the index of a word as we one-hot encode it. Let’s now focus on our sentence: we break it up into words and one-hot encode it — that is, we populate a tensor with one one-hot-encoded vector per word. We create an empty vector and assign the one-hot-encoded values of the word in the sentence:"
      ],
      "metadata": {
        "id": "kghr5N14EAYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_t = torch.zeros(len(words_in_line), len(word2index_dict))\n",
        "for i, word in enumerate(words_in_line):\n",
        "  word_index = word2index_dict[word]\n",
        "  word_t[i][word_index] = 1\n",
        "  print('{:2} {:4} {}'.format(i, word_index, word))\n",
        "\n",
        "print(word_t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx99yy8DD94M",
        "outputId": "daf52114-67ef-48cf-cf7b-b1d79d80dc50"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0 3394 impossible\n",
            " 1 4305 mr\n",
            " 2  813 bennet\n",
            " 3 3394 impossible\n",
            " 4 7078 when\n",
            " 5 3315 i\n",
            " 6  415 am\n",
            " 7 4436 not\n",
            " 8  239 acquainted\n",
            " 9 7148 with\n",
            "10 3215 him\n",
            "torch.Size([11, 7261])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, tensor represents one sentence of length 11 in an encoding space of size 7,261, the number of words in our dictionary. Figure 4.6 compares the gist of our two options for splitting text (and using the embeddings we’ll look at in the next section). \n",
        "\n",
        "\n",
        "The choice between character-level and word-level encoding leaves us to make a trade-off. In many languages, there are significantly fewer characters than words: representing characters has us representing just a few classes, while representing words requires us to represent a very large number of classes and, in any practical application, deal with words that are not in the dictionary. On the other hand, words convey much more meaning than individual characters, so a representation of words is considerably more informative by itself. Given the stark contrast between these two options, it is perhaps unsurprising that intermediate ways have been sought, found, and applied with great success: for example, the byte pair encoding method6 starts with a dictionary of individual letters but then iteratively adds the most frequently observed pairs to the dictionary until it reaches a prescribed dictionary size. Our example sentence might then be split into tokens like this:\n",
        "\n",
        "`?Im|pos|s|ible|,|?Mr|.|?B|en|net|,|?impossible|,|?when|?I|?am|?not|➥\n",
        "?acquainted|?with|?him`\n",
        "\n",
        "![](https://raw.githubusercontent.com/ImranNust/DeepLearningWithPyTorch/main/Images/Figure46.png)\n",
        "\n",
        "For most things, our mapping is just splitting by words. But the rarer parts—the capitalized Impossible and the name Bennet—are composed of subunits."
      ],
      "metadata": {
        "id": "dotJJLIFEsje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h2> <center> <b> <u> Text embeddings </u> </b> </center> </h2>\n",
        "\n",
        "One-hot encoding is a very useful technique for representing categorical data in tensors. However, as we have anticipated, one-hot encoding starts to break down when the number of items to encode is effectively unbound, as with words in a corpus. In just one book, we had over 7,000 items!\n",
        "\n",
        "We certainly could do some work to deduplicate words, condense alternate spellings, collapse past and future tenses into a single token, and that kind of thing. Still, a general-purpose English-language encoding would be huge. Even worse, every time we encountered a new word, we would have to add a new column to the vector, which would mean adding a new set of weights to the model to account for that new vocabulary entry — which would be painful from a training perspective.\n",
        "\n",
        "How can we compress our encoding down to a more manageable size and put a\n",
        "cap on the size growth? Well, instead of vectors of many zeros and a single one, we can use vectors of floating-point numbers. A vector of, say, 100 floating-point numbers can indeed represent a large number of words. The trick is to find an effective way to map individual words into this 100-dimensional space in a way that facilitates downstream learning. This is called an **embedding.**\n",
        "\n",
        "In principle, we could simply iterate over our vocabulary and generate a set of 100 random floating-point numbers for each word. This would work, in that we could cram a very large vocabulary into just 100 numbers, but it would forgo any concept of distance between words based on meaning or context. A model using this word embedding would have to deal with very little structure in its input vectors. An ideal solution would be to generate the embedding in such a way that words used in similar contexts mapped to nearby regions of the embedding.\n",
        "\n",
        "Well, if we were to design a solution to this problem by hand, we might decide to build our embedding space by choosing to map basic nouns and adjectives along the axes. We can generate a 2D space where axes map to nouns—`fruit` (0.0-0.33), `flower` (0.33-0.66), and `dog` (0.66-1.0)—and adjectives—`red` (0.0-0.2), `orange` (0.2-0.4), `yellow` (0.4-0.6), `white` (0.6-0.8), and `brown` (0.8-1.0). Our goal is to take actual fruit, flowers, and dogs and lay them out in the embedding.\n",
        "\n",
        "As we start embedding words, we can map `apple` to a number in the `fruit` and `red` quadrant. Likewise, we can easily map `tangerine`, `lemon`, `lychee`, and `kiwi` (to round out our list of colorful fruits). Then we can start on flowers, and assign `rose`, `poppy`, `daffodil`, `lily`, and … Hmm. Not many brown flowers out there. Well, `sunflower` can get `flower`, `yellow`, and `brown`, and then `daisy` can get `flower`, `white`, and `yellow`. Perhaps we should update `kiwi` to map close to `fruit`, `brown`, and `green`. For `dogs` and `color`, we can embed `redbone` near `red`; uh, `fox` perhaps for `orange`; `golden retriever` for `yellow`, `poodle` for `white`, and … most kinds of `dogs` are `brown`.\n",
        "\n",
        "![](https://raw.github.com/ImranNust/DeepLearningWithPyTorch/main/Images/Figure47.png)\n",
        "\n",
        "Now our embeddings look like the figure shown above. While doing this manually isn’t really feasible for a large corpus, note that although we had an embedding size of 2, we described 15 different words besides the base 8 and could probably cram in quite a few more if we took the time to be creative about it.\n",
        "\n",
        "As you’ve probably already guessed, this kind of work can be automated. By processing a large corpus of organic text, embeddings similar to the one we just discussed can be generated. The main differences are that there are 100 to 1,000 elements in the embedding vector and that axes do not map directly to concepts: rather, conceptually similar words map in neighboring regions of an embedding space whose axes are arbitrary floating-point dimensions.\n",
        "\n",
        "\n",
        "While the exact algorithms used are a bit out of scope for what we’re wanting to focus on here, we’d just like to mention that embeddings are often generated using neural networks, trying to predict a word from nearby words (the context) in a sentence. In this case, we could start from one-hot-encoded words and use a (usually rather shallow) neural network to generate the embedding. Once the embedding was available, we could use it for downstream tasks.\n",
        "\n",
        "One interesting aspect of the resulting embeddings is that similar words end up not only clustered together, but also having consistent spatial relationships with other words. For example, if we were to take the embedding vector for `apple` and begin to add and subtract the vectors for other words, we could begin to perform analogies like `apple` - `red` - `sweet` + `yellow` + `sour` and end up with a vector very similar to the one for `lemon`.\n",
        "\n",
        "More contemporary embedding models—with BERT and GPT-2 making headlines even in mainstream media—are much more elaborate and are context sensitive: that is, the mapping of a word in the vocabulary to a vector is not fixed but depends on the surrounding sentence. Yet they are often used just like the simpler classic embeddings we’ve touched on here."
      ],
      "metadata": {
        "id": "7jD7wlqCTZYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "\n",
        "<h2> <center> <b> <u> Text embeddings as a blueprint </u> </b> </center> </h2> \n",
        "\n",
        "Embeddings are an essential tool for when a large number of entries in the vocabulary have to be represented by numeric vectors. But we won’t be using text and text embeddings in this book, so you might wonder why we introduce them here. We believe that how text is represented and processed can also be seen as an example for dealing with categorical data in general. Embeddings are useful wherever one-hot encoding becomes cumbersome. Indeed, in the form described previously, they are an efficient way of representing one-hot encoding immediately followed by multiplication with the matrix containing the embedding vectors as rows.\n",
        "\n",
        "\n",
        "In non-text applications, we usually do not have the ability to construct the embeddings beforehand, but we will start with the random numbers we eschewed earlier and consider improving them part of our learning problem. This is a standard technique — so much so that embeddings are a prominent alternative to one-hot encodings for any categorical data. On the flip side, even when we deal with text, improving the prelearned embeddings while solving the problem at hand has become a common practice.\n",
        "\n",
        "When we are interested in co-occurrences of observations, the word embeddings we saw earlier can serve as a blueprint, too. For example, recommender systems—customers who liked our book also bought …—use the items the customer already interacted with as the context for predicting what else will spark interest. Similarly, processing text is perhaps the most common, well-explored task dealing with sequences; so, for example, when working on tasks with time series, we might look for inspiration in what is done in natural language processing."
      ],
      "metadata": {
        "id": "2dc1gATOsegU"
      }
    }
  ]
}